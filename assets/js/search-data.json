{
  
    
        "post0": {
            "title": "Reading List: Resources I've Found Helpful (Pinned Post)",
            "content": "Introduction . This is a non-exhaustive list of resources I&#39;ve found instructive and insightful. They run the gamut. Some are technical in focus, while others are general reading. Topics range from AI (or some subset of it) only, to law and AI. I will update this list periodically. . If there are any resources you&#39;d recommend, please let me know! . Text Books . Deep Learning Illustrated, John Krohn (Addison-Wesley Professional, 2019) . | Grokking Algorithms, Aditya Bhargava (Manning Publications, 2016) . | Grokking Artificial Intelligence Algorithms, Rishal Hurbans (Manning Publications, 2020) . | Grokking Deep Learning, Andrew Trask (Manning Publications, 2019) . I cannot say enough good things about this book. This is the resource that made the fundamentals of deep learning &quot;click&quot; for me. It also focuses on coding neural networks from scratch in Python, rather than using a library (e.g. Keras). I found this approach very insightful. . | Introduction to Computational Programming Using Python with Application to Understanding Data, John V. Guttag, (MIT Press, 2016) . | Python for Data Analysis, Wes McKinney (O&#39;Reilly, 2014) . | . Online Courses . Build A Machine Learning Model with Python (Skills Path), Codecademy . | Build Deep Learning Models with TensorFlow (Skills Path), Codecademy . | Computer Science (Career Path), Codecademy . | . Websites . Machine Learning Mastery | Real Python | . General Reading re: AI . Artificial Intelligence: A Guide for Thinking Humans, Melanie Mitchell (Picador, 2019) . | Possible Minds: Twenty-Five Ways of Looking at AI, ed. John Brockman (Penguin, 2020) . | The Master Algorithm, Pedro Domingos (Basic Books, 2018) . | . Law &amp; AI: Papers . Algorithms As Legal Decisions: Gender Gaps and Canadian Employment Law in the 21st Century, Niblett, Anthony, Available at SSRN . | Predicting Economic Substance Cases with Machine Learning, Alarie, Benjamin and Aidid, Abdi, Journal of Tax Practice &amp; Procedure, 2020, Available at SSRN . | Using Text Analytics to Predict Litigation Outcomes, Alexander, Charlotte and Al Jadda, Khalifeh and Feizollahi, Mohammed Javad and Tucker, Anne M., Law as Data: Computation, Text, and the Future of Legal Analysis (under contract, Santa Fe Institute Press, Michael Livermore &amp; Daniel Rockmore, eds., 2018, Forthcoming)., Georgia State University College of Law, Legal Studies Research Paper No. 2018-13, Available at SSRN . | .",
            "url": "http://www.litkm.com/artificial%20intelligence/general/law/machine%20learning/python/2021/06/11/Reading_List.html",
            "relUrl": "/artificial%20intelligence/general/law/machine%20learning/python/2021/06/11/Reading_List.html",
            "date": " • Jun 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Using Machine Learning to Find Analogues in Legal Data",
            "content": "Introduction . Have we done a budget for a case like this before? What other matters have we handled with a similar profile to this one? In which other files with characteristics like this one have we brought a summary judgment motion? . This type of question arises regularly in a law firm. Answers can sometimes be challenging. Searching databases via keywords or filters is often inefficient at best, and inadequate at worst. Hence the impetus to apply machine learning to this problem. . One approach worth exploring is the k-nearest neighbor algorithm. This is among the oldest and simplest machine learning algorithms. It is typically used for regression and classification problems. But it can also be used just to find analogues in a dataset. . In this post, I introduce the NearestNeighbors() class from the scikit-learn library. This class takes an example, x, and simply returns the element(s) in the dataset that most closely resembles it. . The K-Nearest Neighbors Algorithm . In a nutshell, KNN processes the elements of a dataset and plots each of them in a multi-dimensional space. The algorithm calculates the location of each element based on its features. The distances between these elements are then used to make predictions. . There is no shortage of excellent tutorials on the KNN algorithm. My favourites include Develop k-Nearest Neighbors in Python from Scratch on Machine Learning Mastery and The k-Nearest Neighbors Algorithm in Python on Real Python. Codecademy also addresses KNN in its Build a Machine Learning Model with Python skills path. . All of the tutorials I have studied address using KNN for classification or regression problems. For instance, . What kind of flower is this example likely to be given the measurements of particular features? (Classification) | What is the likely market price of this example of real estate given its various features such as square footage, number of bedrooms, etc? (Regression) | . These tutorials also tend either to focus on coding the KNN algorithm from scratch in Python, or using the KNeighborsClassifier() and KNeighborsRegressor() classes from scikit-learn. All of these approaches rely on calculating &quot;nearest neighbors&quot;, but this is not the output the algorithm produces. Rather, the algorithm provides a prediction regarding a classification (x is an example of category y) or a value for regression (x is valued at $y). . But what if the output you want is simply to know which elements in the dataset are the nearest neighbors to x? You could code this from scratch. Or you could use the NearestNeighbor() class from scikit-learn! . NearestNeighbors in Action . Let&#39;s apply NearestNeighbor() to a very simple dataset. . In the below snippet, we import our dependencies and create our sample dataset: five elements (rows), each with five features (columns). To create this dataset, we first define a dictionary with five keys (A to E), with each key associated with a list containing five values. This dictionary is then converted into a pandas DataFrame. . import pandas as pd from sklearn.neighbors import NearestNeighbors samples = {&#39;A&#39;: [10, 20, 30, 40, 50], &#39;B&#39;: [10, 20, 30, 40, 50], &#39;C&#39;: [10, 20, 30, 40, 50], &#39;D&#39;: [10, 20, 30, 40, 50], &#39;E&#39;: [10, 20, 30, 40, 50]} dataset = pd.DataFrame(samples) print(dataset) . A B C D E 0 10 10 10 10 10 1 20 20 20 20 20 2 30 30 30 30 30 3 40 40 40 40 40 4 50 50 50 50 50 . Now suppose we have a new element, x, and we want to know which element in our dataset is the most similar (or nearest neighbor) to it. In this example, x has a value of 9 in each column. . In the next snippet, we create a new DataFrame comprising the features for x. Then, we instantiate an instance of NearestNeighbor(), assign it to the variable neigh, and fit it to the dataset. . Finally, we run our query using the kneighbors() method to find the nearest neighbor to x. . x = {&#39;A&#39;: [9], &#39;B&#39;: [9], &#39;C&#39;: [9], &#39;D&#39;: [9], &#39;E&#39;: [9]} x = pd.DataFrame(x) neigh = NearestNeighbors(n_neighbors=1) neigh.fit(dataset) print(neigh.kneighbors(x)) . (array([[2.23606798]]), array([[0]])) . NearestNeighbor() returns 2.236 and 0, indicating the nearest neighbor to x is the element at row 0 of our dataset, and the distance between these elements is 2.236. . Let&#39;s try another example, y. This example has a value of 60 in each column. . y = {&#39;A&#39;: [60], &#39;B&#39;: [60], &#39;C&#39;: [60], &#39;D&#39;: [60], &#39;E&#39;: [60]} y = pd.DataFrame(y) print(neigh.kneighbors(y)) . (array([[22.36067977]]), array([[4]])) . As expected, the nearest neighbor to y is the element at row 4 of our dataset. . You can find more information about NearestNeighbor() in the documentation for scikit-learn. . Applying KNN to Legal Data . Admittedly, the preceding example is very abstract when considered in relation to how KNN can be implemented with legal data. Imagine each row of the dataset is one case, and each column is one feature regarding that case (e.g., jurisdiction, case type, etc.) . No sugarcoating: one of the trickiest parts to using KNN to find analogues in legal data is first putting the dataset together. This will usually involve a lot of wrangling. But, hey, it&#39;s good practice! . Final Thoughts . Have you experimented with using KNN on legal data? Hit me up! Let&#39;s compare notes. .",
            "url": "http://www.litkm.com/python/machine%20learning/scikit-learn/knn/law/2021/05/05/KNN_and_Legal_Data_v2.html",
            "relUrl": "/python/machine%20learning/scikit-learn/knn/law/2021/05/05/KNN_and_Legal_Data_v2.html",
            "date": " • May 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Cyborg Barristers: A Golden Age for Oral Advocacy",
            "content": "Introduction . When I was a baby litigator, I enjoyed hearing stories from senior counsel about courtroom advocacy &quot;back in the day.&quot; Oral advocacy usually featured prominently. What the lawyer said, and not so much what the lawyer wrote, was decisive. Modern civil litigation, however, relies mainly on written advocacy. . In the years to come, AI-based software tools will change litigation. Counterintuitively, I think one of these changes will be to reinvigorate the importance of oral advocacy before the Canadian courts. . Written Advocacy Has Replaced Oral Advocacy . Oral advocacy is a fading star. In a recent treatise titled Some Thoughts on Legal Writing and Written Advocacy, Justice Stratas of the Canadian Federal Court of Appeal writes: . Once upon a time, civil litigation depended exclusively on oral advocacy. That time is gone. When courts decide public law and civil cases, more and more they rely on written advocacy. . Whatever other reasons there are for this transition, complexity and volume must be chief among them. Cases are more complex than ever. They involve more documents, more experts, more law; more of everything, in short. And there are more of all of these cases! . To cope, lawyers rely on the written word to state everything they will not have time to say while before the court in person. It&#39;s a kind of buffering, if you will. . AI in Litigation Will Mainly Influence Written Advocacy . AI is already changing litigation. We see this in e-discovery, but increasingly in research, litigation analytics, and knowledge management. . To date, all AI enhanced software tools are examples of so-called narrow artificial intelligence. That is, these tools address problems or tasks relating to one specific context only. At best, narrow AI systems can be combined to increase breadth. For instance, e-discovery platforms integrate a range of AI algorithms to process data in different ways. There is no &quot;generalized&quot; legal AI that can assist with e-discovery on one day, research into case law on the next, and then prepare a first draft of a factum on the day after. . Since there is no generalized legal AI, the legal AI industry is developing in several different directions. Artificial Lawyer recently published a fascinating graphic depicting the ways legal AI has grown into branches and sub-branches. For instance, litigation analytics is branching into categories of tools addressing case law research, participant analysis (e.g. judge analytics), and drafting. There is also an emerging area of software geared towards generating drafts of legal documents; so far, there is a branch for patent applications and another for discovery responses. All AI algorithms require data. Wherever on the legal AI &quot;tree&quot; a tool may be situated, this data is mainly the written word. All of these AI systems also output data, likewise primarily in written form. This makes legal AI especially useful for lawyers’ written work product, even when an AI system is not explicitly designed for this purpose. . How AI Will Enhance Written Advocacy . Forecasting the future is obviously risky. Both the timeline, and the actual AI tools of the future, remain unclear. That said, I think the tools presently available, and under development, suggest the trajectory. . There is already intelligent software that takes legal questions (rather than just search terms), scours the case law, and provides a written response. These tools will likely improve to the point where they can provide accurate statements of law nearly ready to be dropped into a factum. . There are also tools geared towards generating full-blown legal documents. In time, I imagine we will see tools that will generate substantive first drafts of a range of documents, including notices of motion, affidavits, and factums. When the lawyer reviews these drafts, this software (or another one) will also provide options for optimizing the drafting for style and substance. . This legal drafting software will learn from the firm&#39;s own work product, work product the firm receives from opposing counsel, and anything else that is publicly available. It will further improve as it interacts with the firm&#39;s lawyers. Drafting legal documents may begin to resemble a dialogue of sorts. The machine will predict appropriate language, and the lawyer will confirm, reject, or revise. Over time, the machine should get better - much like a junior associate eventually acquires the experience to become a seasoned corner office partner. . AI tools may also help simplify cases. Even the most complex cases usually boil down to a handful of truly key documents and principles. An AI tool trained on a firm&#39;s experience with similar cases, and the case law, might assist lawyers in focusing their written submissions with better precision. . Two streams are now converging: the present emphasis on written advocacy will combine with AI litigation software, and this software will mainly improve lawyers&#39; written work. This is where the impact will be felt first and foremost. But one indirect result of this combination will be to reinvigorate the importance of oral advocacy. Let me explain. . As AI Enhances Written Advocacy, Oral Advocacy Will Become More Important . At least for the foreseeable future, all forthcoming AI tools for litigation will be narrow AI systems. They will each do one thing, or multiple closely related things, only. Eventually, some of them should also do this one thing (or things) really well. . Most, if not all, of these tools will mainly enhance written advocacy. As I described, they will facilitate legal research, prepare drafts of written submissions, and then assist with editing. Eventually, we should see fewer differences in style and quality. These tools will probably also help with distilling even the most complex cases into key components, so court filings become concise. . Once uptake among lawyers becomes widespread, the quality of written submissions of every kind will improve across the market. Although adopting new technology occurs slowly in the legal industry, its steady march is undeniable. The very best written advocacy Canada&#39;s lawyers presently have on offer should eventually become the baseline. . As the standard for written advocacy levels off on a high plateau, oral advocacy will become more important. Based on present trends, AI is not poised to enhance oral advocacy in any way comparable to written advocacy. At the same time, court hearings, whether in-person or remote, are fundamental to civil litigation in Canada. Our system also remains adversarial. There is no serious sign of either of these features changing. If written advocacy becomes generally less persuasive (because it is all so good!), then litigators must focus their efforts more on the only avenue of persuasion that remains: oral advocacy. This is where the advocate, with no (or limited) assistance from a machine, may yet make a difference in pleading the client&#39;s case. . Final Thoughts . In this post, I&#39;ve tried to sketch some possibilities for AI-based litigation tools given the evident trends. Whatever specific tools are actually developed, I believe we can be confident they will mainly enhance lawyers&#39; written work, and thereby their written advocacy. While one might be tempted to conclude this will further diminish the importance of oral advocacy, I think the opposite is likely. Current trends in AI are cuing oral advocacy up to become more important than ever. Maybe it will not define civil litigation as it did in the days of yore, but I think it could still become decisive in a way it has not been for awhile now.1 . 1. That is, of course, until AI researchers achieve a &quot;master legal algorithm&quot; and courts, featuring judges in the flesh and blood, are no longer required!↩ .",
            "url": "http://www.litkm.com/artificial%20intelligence/machine%20learning/litigation/law/lawyering/general/2021/03/29/Cyborg_Barristers.html",
            "relUrl": "/artificial%20intelligence/machine%20learning/litigation/law/lawyering/general/2021/03/29/Cyborg_Barristers.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "How to Load Text Files as a Corpus for NLTK in Google Colab",
            "content": "Introduction . Most tutorials on NLTK presume you are working locally as opposed to in a cloud environment such as Google Colab. . In this blog post, I address using Google Colab to load text files for use as a corpus from: . within Google Colab; and | Google Drive. | When I looked on the web, I did not see any articles addressing this directly. Hence this brief post. . Loading from within Google Colab . First, you must upload the text files into your Colab environment. . Click on the file icon located on the left side of the screen. Navigate the file structure to where you wish to store the files. . By default, your Colab environment will have a /content subfolder. For this post, I created a subfolder called /content/textfiles. This is where I then uploaded the text files for the corpus. To upload, right click on the folder where you wish the files to be placed. . In the below screen shot, you see the file structure and the &quot;test&quot; text files I uploaded. . . Now we are ready to load the text files as a corpus. From hereon, the process is essentially the same as if you were working locally. . For the limited purposes of this tutorial, the below dependencies are required. . import nltk from nltk.corpus import PlaintextCorpusReader . NLTK contains a class called PlaintextCorpusReader() for creating a corpus from text files. . In the below example, we assign the directory where the files are located to a variable (corpus_root). . We then instantiate an instance of PlaintextCorpusReader() and assign it to the variable corpus. The parameters indicate where to find the text files, and which files to include (in this example, all of them). . Finally, to confirm the corpus has been constituted, we call the fileids() method to list the files contained within the corpus. . corpus_root = &#39;/content/textfiles&#39; corpus = PlaintextCorpusReader(corpus_root, &#39;.*&#39;) corpus.fileids() . [&#39;test_data_1.txt&#39;, &#39;test_data_2.txt&#39;, &#39;test_data_3.txt&#39;] . If you are using a free Colab account, each time your disconnect from the runtime environment your files will be deleted. To avoid this, either upgrade your Colab account or use Google drive. . Loading from Google Drive . The process for creating a corpus from text files located on your Google Drive is similar to the above. These instructions assume you are using the same Google account for both Colab and Google Drive. . First, upload the text files into your Google Drive. Take note of the directory. . In addition to the dependencies listed above, one more is required. This is to mount your Google Drive in your Colab environment. Then call the the mount method and follow the instructions that result. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . Now your Google Drive is mounted. The balance of the process is the same as above, i.e. as if you were working with the files directly in your Colab environment. Make sure to revise the file path as necessary. . corpus_root = &#39;/content/drive/MyDrive/Datasets&#39; corpus = PlaintextCorpusReader(corpus_root, &#39;.*&#39;) corpus.fileids() . [&#39;test_data_1.txt&#39;, &#39;test_data_2.txt&#39;, &#39;test_data_3.txt&#39;] . Now you are ready to start processing your corpus! . If you have any questions, please feel free to reach out. .",
            "url": "http://www.litkm.com/natural%20language%20processing/preprocessing%20text/nltk/nlp/google%20colab/2021/03/12/Loading_Text_for_NLTK_in_Google_Colab.html",
            "relUrl": "/natural%20language%20processing/preprocessing%20text/nltk/nlp/google%20colab/2021/03/12/Loading_Text_for_NLTK_in_Google_Colab.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Parsing a Neural Network for Predicting SCOTUS Judging",
            "content": "Introduction . In a recent posted titled Using Artificial Intelligence to Predict SCOTUS Judging, I discussed a machine learning model I used to make predictions regarding Justice Brennan&#39;s voting record on the Supreme Court of the United Status (SCOTUS). This model is a neural network, coded in Python, and uses the Keras framework. In the present post, I review the code, line-by-line, and explain it. . This post focuses on code and not the dataset. For more information regarding the latter, please see my earlier post (though it&#39;s worth repeating I obtained the dataset from from Prof. Alschner&#39;s great site, Data Science For Lawyers). . Further, as noted in the README for this blog, I&#39;m assuming the reader has a basic level of familiarity with object oriented programming and common Python libraries. . That said, I&#39;m also aiming to write this in a way so that a general reader will still be able to follow along (more or less). If you are interested in this content but I&#39;m assuming too much background knowledge, please let me know. I&#39;d be happy to explain further. . For reference, I developed the model using Google Colab. . Workflow . Before delving into the details, this is the workflow underpinning the code: . Import modules | Load data | Define the training set and testing set | Preprocess the data | Define the model | Run the model | Report on the results | Modules . We begin our script by loading the modules we require. These are our tools to preprocess the data and assemble the model. This model does not involve coding any functions or classes from scratch. We are not building any new tools. Instead, we import everything we need. As a result, this script is remarkably short. . import pandas as pd from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, InputLayer from sklearn.metrics import classification_report from tensorflow.keras.utils import to_categorical import numpy as np . In brief, we import libraries or portions thereof from: . pandas - for loading the data regarding Justice Brennan&#39;s voting record | Scikit-learn - for preprocessing the data | NumPy - to help with the reporting; and, of course | Keras - the framework this model uses | . Data . At this step, our objective is to load the data into a pandas DataFrame. Once in this format, we can begin preprocessing it for the model to analyze. . dataset = &#39;https://raw.githubusercontent.com/litkm/WJBrennan-Voting/main/WJBrennan_voting.csv&#39; dataset = pd.read_csv(dataset) . We obtain our data from a CSV file. For convenience, I uploaded it to GitHub (in raw format). We then create a variable and assign it to the web address where the file is located. . Next, we pass this variable into the the pandas read_csv() method to load the CSV file as a DataFrame, and reassign the variable for the CSV file to now be the variable for the DataFrame. . Training Set and Testing Set . To train this model, we must split the data into training and test sets. The model &quot;learns&quot; from the training data; during the learning phase, the test data is excluded from review. Once it completes a learning phase, the model switches to a test phase, where it evaluates its predictive capacity (i.e. how well it learned from the training data) using the test data. . This model uses supervised learning. Often, this is framed in terms of x and y variables: . x represents the data inputted into the model. These inputs are sometimes called features. | y represents an outcome the model is to predict. This is sometimes called the target variable or label. | . During training, the model processes the features and uses them to make predictions. These predictions are compared against the corresponding label. The outcome of this comparison is a &quot;supervisory signal&quot;, i.e. whether the prediction was correct or not; and, if not, by how much. The model then uses this &quot;signal&quot; to recalibrate with the objective of improving its predictive capacity. . With this in mind, we need to identify our x and y variables in the Justice Brennan dataset. The first five rows of the dataset is reproduced below: . . Our target variable for this model is the outcome identified in the &quot;vote&quot; column, i.e. whether Justice Brennan voted with the majority or the minority. This is what we want to predict in respect of each row (where each row represents one SCOTUS case). . The data in the preceding columns comprises the information we intend to input into the model, and which the model will use to predict the target variables. . During training, the model will review each row of features, case by case, and make a prediction relating to that case. This prediction will then be compared to the label for that case, i.e. whether Brennan voted with the majority or not. This comparison provides the supervisory signal. Based on the result, the model recalibrates and proceeds to the next row in the dataset, i.e. the next case. . After the model cycles through the training dataset, it then evaluates its predictive capacity against the test dataset. . To provide the foregoing, we must split the original dataset into four subsets: . X_train - the set of features for training | Y_train - the corresponding set of labels for training | X_test - the set of features for testing | Y_test - the correspondence set of labels for testing | We accomplish this in a few steps. . First, we split the DataFrame into features and labels: . y = dataset[&#39;vote&#39;] x = dataset[[&#39;term&#39;, &#39;petitioner&#39;, &#39;respondent&#39;, &#39;jurisdiction&#39;, &#39;caseOrigin&#39;, &#39;caseSource&#39;, &#39;certReason&#39;, &#39;issue&#39;, &#39;issueArea&#39;]] . We now have a DataFrame assigned to a variable called y, and it contains all of the labels. . We now also have a DataFrame assigned to a variable called x, and it contains all of the features. . Next, we further split these two DataFrames into training sets (X_train and Y_train) and test sets (X_test and Y_test). To do this, we use the test_train_split() function from the sci-kit learn library: . X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.3, random_state = 0) . As you see, test_train_split takes several parameters: . x - representing DataFrame of features to be split | y - representing the DataFame of labels to be split | test_size - this parameter specifies the size of the test set; in this instance, we allocate 30% of the dataset for testing | random_state - this parameter controls the shuffling (randomization) applied to the data before applying the split | . When calling this function, we assign the data subsets to the variables X_train, Y_train, X_test, and Y_test. Now we are ready to proceed to the next stage. . Preprocessing the Data . Before the data can be fed into the model, it must be preprocessed for optimal results. In this instance, we need to: . Scale the features | Convert the labels from categories to integers | As you can see from the printout of the dataset above, all of the features are represented using numbers. Based on the first five rows alone, we note dissimilarity; for example: . respondent - ranges from 3 to 369 | jurisdiction - ranges from 1 to 2 | issue - ranges from 40,070 to 120,020 | . If we were to dive into the rest of the dataset, we would see this dissimilarity is representative. Each of the columns has a different range, mean, etc. . When we scale the features, the model recalculates the numbers that comprise the features so that there is zero mean variance between them. . To accomplish this, we use another tool from the sci-kit learn library, namely the ColumnTransformer() class. . columns_for_standard = [&#39;term&#39;, &#39;petitioner&#39;, &#39;respondent&#39;, &#39;jurisdiction&#39;, &#39;caseOrigin&#39;, &#39;caseSource&#39;, &#39;certReason&#39;, &#39;issue&#39;, &#39;issueArea&#39;] ct = ColumnTransformer([(&#39;numeric&#39;, StandardScaler(), columns_for_standard)]) X_train = ct.fit_transform(X_train) X_test = ct.transform(X_test) . For convenience, we first assign the features we wish to scale to a variable called columns_for_standard. This variable is used in the next line of code. . Then, we create a ColumnTransformer() object assigned to the variable ct. When initializing this object, we configure the form of scaling and specify the features to be scaled. . Next, we use the related class methods, fit_transform() and transform(), to apply the scaler to the features. Each of these methods produces NumPy arrays comprising the now scaled features. These arrays are converted back into DataFrames and assigned, respectively, to our X_train and X_test variables. Done! We&#39;ve scaled our features. . Now to convert the labels from categories to integers (whole numbers) and ensure they, too, are scaled. On review of the &quot;vote&quot; column above, you will note there are no numbers. Rather, there is only the word &quot;majority&quot;. Throughout the dataset, the &quot;vote&quot; column only ever has the word &quot;majority&quot; or &quot;minority&quot; under it. To make these &quot;categories&quot; digestable for the model, we must convert them into integers and scale them. . Here again, we use a tool from the sci-kit learn library, namely the LabelEncoder() class along with the pandas class method astype(). . le = LabelEncoder() Y_train = le.fit_transform(Y_train.astype(str)) Y_test = le.transform(Y_test.astype(str)) Y_train = to_categorical(Y_train) Y_test = to_categorical(Y_test) . We first initialize a LabelEncoder() object and assign it to the variable le. . Then, for each of Y_train and Y_test, we call the astype() method to convert the categories (i.e. words) into integers, while calling the LabelEncoder() class methods fit_transform() and transform(), as applicable, to scale each of these DataFrames, too. . We must then use a Keras function called to_categorical() to convert the integers in Y_train and Y_test into a form called one-hot-encodings, which the model requires to factor properly for the &quot;supervisory signals&quot; discussed above. . At this point, we are finally ready to create the neural network itself. . Define the Neural Network . To create the neural network, we use Keras&#39; Sequential() class. This is one of the most popular types of models and provides for adding layers to the neural network, one after the other in a straightforward way. . We need to sepecify: . An input layer | Any hidden layers | The output layer | Below is a diagram of a simple neural network: . . The model we are building is also very simple and is likewise commprised of only three layers: an input layer, one hidden layer, and then the output layer. The code to create this is below. . model = Sequential() model.add(InputLayer(input_shape=(X_train.shape[1],))) model.add(Dense(10, activation=&#39;relu&#39;)) model.add(Dense(2, activation=&#39;softmax&#39;)) . We invoke the Sequential() class and assign it to the variable model. Then, we add the input layer. We configure it (the input_shape parameter) so that there is one input unit (&quot;synthetic neuron&quot;) for each type of feature in the X_train DataFrame. The example model in the diagram above has two input units (the circles in the input layer). By contrast, this model has nine input units because the dataset has nine different types of features. . Then, we add the hidden layer and the output layer. The integers &quot;10&quot; and &quot;2&quot; indicate the number of neurons in each layer. The activation parameter is a setting that configures some of the math the model performs each time a row (i.e. case) from the dataset is passed through the neural network. . Now we need to &quot;compile&quot; our model. Among other things, the below line of code further configures the math the model uses across all layers of the neural network when processing the dataset. . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . Running the Model . At this stage, we are finally ready to run our model. To do this, we use Keras&#39; fit() method with the following paramaters: . X_train - the training features | Y_train - the training labels | epochs - the numbers of times the model cycles through the entire dataset | batch_size - the number of rows from the dataset (i.e. cases) the model will feed through the neural network before recalibrating in response to the supervisory signals | verbose - set to 1 tells the model to print its progress to the screen | validation_data - indicates the test features and test labels to use during the testing phase | . model.fit(X_train, Y_train, epochs=5, batch_size=8, verbose=1, validation_data=(X_test, Y_test)) . Epoch 1/5 416/416 [==============================] - 1s 2ms/step - loss: 0.4487 - accuracy: 0.7953 - val_loss: 0.4750 - val_accuracy: 0.7795 Epoch 2/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4468 - accuracy: 0.7929 - val_loss: 0.4782 - val_accuracy: 0.7767 Epoch 3/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4476 - accuracy: 0.7908 - val_loss: 0.4727 - val_accuracy: 0.7767 Epoch 4/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4476 - accuracy: 0.7932 - val_loss: 0.4746 - val_accuracy: 0.7704 Epoch 5/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4479 - accuracy: 0.7908 - val_loss: 0.4770 - val_accuracy: 0.7760 . &lt;tensorflow.python.keras.callbacks.History at 0x7fabab0d4e50&gt; . The output above suggests the model learns from the data fairly quickly. After one round of training, training accuracy is ~80% and testing accuracy is ~77%. Sometimes when I&#39;ve run this model, training accuracy after the first round has been around ~62%; testing accuracy has also been lower than this example. However, by the second round the model seems consistently to max out at around ~79%-80% training accuracy and ~77%-78% testing accuracy, per around. . I&#39;ve played around a bit with the hyperparameters (i.e. the configuration of the model in terms of layers, number of neurons, number of layers, epochs, etc.), and have yet to improve performance materially. So I&#39;ve stuck with the simplest implementation of this model for this post. . Reporting the Results . To evaluate the performance of the model over all epochs, we can use Keras&#39; evaluate() method. . loss, acc = model.evaluate(X_test, Y_test, verbose=0) print(&quot;Loss:&quot;, loss, &quot;Accuracy:&quot;, acc) . Loss: 0.4769740104675293 Accuracy: 0.7759831547737122 . These lines of code evaluate the model using the testing data (X_test) and the testing labels (Y_test), and outputs (prints) the results. Overall, the model accurately predicted Justice Brennan&#39;s vote with ~78% accuracy. . The other Loss number is a calculation relating to how far off the mark the model&#39;s predictions were, overall. The closer this number is to 0, the better. This number can also be used to tune the model and further details will have to wait for another post. . Final Thoughts . My comments on the foregoing code glosses over a lot of detail, particularly in terms of the calculcations the model performs. I am planning to address this in a future post. . Candidly, I still don&#39;t understand everything that is going on in the code discussed. But it seems to work. . Thanks for reading! Did I make a mistake? Does something not make sense? Hit me up. . Appendix . For ease of review, the entire script is set out below. . import pandas as pd from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, InputLayer from sklearn.metrics import classification_report from tensorflow.keras.utils import to_categorical import numpy as np dataset = &#39;https://raw.githubusercontent.com/litkm/WJBrennan-Voting/main/WJBrennan_voting.csv&#39; dataset = pd.read_csv(dataset) y = dataset[&#39;vote&#39;] x = dataset[[&#39;term&#39;, &#39;petitioner&#39;, &#39;respondent&#39;, &#39;jurisdiction&#39;, &#39;caseOrigin&#39;, &#39;caseSource&#39;, &#39;certReason&#39;, &#39;issue&#39;, &#39;issueArea&#39;]] X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.3, random_state = 0) columns_for_standard = [&#39;term&#39;, &#39;petitioner&#39;, &#39;respondent&#39;, &#39;jurisdiction&#39;, &#39;caseOrigin&#39;, &#39;caseSource&#39;, &#39;certReason&#39;, &#39;issue&#39;, &#39;issueArea&#39;] ct = ColumnTransformer([(&#39;numeric&#39;, StandardScaler(), columns_for_standard)]) X_train = ct.fit_transform(X_train) X_test = ct.transform(X_test) le = LabelEncoder() Y_train = le.fit_transform(Y_train.astype(str)) Y_test = le.transform(Y_test.astype(str)) Y_train = to_categorical(Y_train) Y_test = to_categorical(Y_test) model = Sequential() model.add(InputLayer(input_shape=(X_train.shape[1],))) model.add(Dense(10, activation=&#39;relu&#39;)) model.add(Dense(2, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X_train, Y_train, epochs=5, batch_size=8, verbose=1, validation_data=(X_test, Y_test)) loss, acc = model.evaluate(X_test, Y_test, verbose=0) print(&quot;Loss:&quot;, loss, &quot;Accuracy:&quot;, acc) . Epoch 1/5 416/416 [==============================] - 1s 2ms/step - loss: 0.5343 - accuracy: 0.7485 - val_loss: 0.4861 - val_accuracy: 0.7669 Epoch 2/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4716 - accuracy: 0.7776 - val_loss: 0.4800 - val_accuracy: 0.7788 Epoch 3/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4673 - accuracy: 0.7794 - val_loss: 0.4776 - val_accuracy: 0.7809 Epoch 4/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4692 - accuracy: 0.7838 - val_loss: 0.4758 - val_accuracy: 0.7802 Epoch 5/5 416/416 [==============================] - 1s 1ms/step - loss: 0.4497 - accuracy: 0.7912 - val_loss: 0.4755 - val_accuracy: 0.7809 Loss: 0.47549518942832947 Accuracy: 0.7808988690376282 .",
            "url": "http://www.litkm.com/artificial%20intelligence/machine%20learning/deep%20learning/litigation/analytics/python/keras/neural%20networks/2021/02/26/Parsing_Model.html",
            "relUrl": "/artificial%20intelligence/machine%20learning/deep%20learning/litigation/analytics/python/keras/neural%20networks/2021/02/26/Parsing_Model.html",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "A README re: this Blog (Pinned Post)",
            "content": "As I write this, my plan for this blog is to focus mainly on technical concepts and code. Most posts will presume a familiarity with programming in general and, in particular, Python. Mind you, I won&#39;t presume deep familiarity. I can&#39;t. My own understanding is too shallow! . That said, I am aiming to include a law angle as often as possible (especially litigation), and I will draft many of these posts with a general, non-technical reader in mind. So, someone who is involved in law and also interested in artificial intelligence, legal tech, etc. To find these posts efficiently, you can use the &quot;Tags&quot; function at the top right-hand of your screen and look under the tag &quot;general&quot;. . Of course, if you have any questions (or corrections!) about anything you read on this site, I would be grateful to hear from you. You can reach me via the &quot;About Me&quot; page. .",
            "url": "http://www.litkm.com/law/litigation/general/2021/02/19/How_to_Read_this_Blog.html",
            "relUrl": "/law/litigation/general/2021/02/19/How_to_Read_this_Blog.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Using Artificial Intelligence to Predict SCOTUS Judging",
            "content": "Introduction . In Prof. Wolfgang Alschner&#39;s fantastic course, Data Science for Lawyers, Lesson 8 uses machine learning to predict Justice Brennan&#39;s voting record. One of the key questions is: if a machine learning model &quot;studies&quot; information about thousands of Justice Brennan&#39;s cases, how well can it predict the way he voted on other cases? . The lesson reviews several different machine learning algorithms.1 This excercise inspired me to attempt to apply a different machine learning approach to the same dataset; namely, deep learning. In this post, I detail the results of this effort. . Justice Brennan . Per Wikipedia, Justice Brennan (1906 – 1997) was an American lawyer and jurist who served as an Associate Justice of the Supreme Court of the United States (SCOTUS) from 1956 to 1990. He was the seventh-longest-serving justice in Supreme Court history, and known for being a leader of the Court&#39;s liberal wing. . The length of Justice Brennan&#39;s tenure is key for present purposes. Since he sat on SCOTUS for so long, he has a lengthy voting record. This is important for &quot;teaching&quot; a machine learning model effectively (the more data, the better). . The Dataset . The dataset is available online at the course website. The data is from The Supreme Court Database. In this database, court decisions are coded for a variety of variables relating to the identification, chronology, background, substance, and outcome of each case. . The dataset is a simple CSV file. Click to view it in a &quot;raw&quot; format. . Below, the first five entries of dataset are printed out. . . Decoded, the first entry indicates: . The case was heard in 1956 (term) | The petitioner (appellant) was a &quot;bank, savings and loan, credit union, investment company&quot; (petitioner) | The respondent was an &quot;agent, fiduciary, trustee, or executor&quot; (respondent) | The court assumed jurisdiction on the basis of a writ of certiorari (jurisdiction) | The case originated from the Pennsylvania Western U.S. District Court (caseOrigin) | The U.S. Court of Appeals, Third Circuit, was the source of the decision SCOTUS reviewed (caseSource) | SCOTUS granted the writ of certiorari in order to &quot;to resolve important or significant question&quot; (certReason) | The subject matter of the controversy related to &quot;cruel and unusual punishment, death penalty (cf. extra legal jury influence, death penalty)&quot; (issue) | The preceding variable was categorized as relating to federalism (issueArea) | Lastly, Justice Brennan voted with the majority (vote) | . Below, additional information from the dataset is set out. . . For present purposes, the most important information shown here is that the dataset contains 4746 entries, i.e. there is information regarding 4746 cases, including whether Justice Brennan voted with the majority or the minority of the SCOTUS panel. . The Deep Learning Model . Machine learning is a subfield of computer science. The basic objective is to program computers to learn so that they can perform tasks for which they were not explicitly programmed.2 . There are many approaches to machine learning, of which deep learning is only one. This approach is based on artificial neural networks, which are a kind of algorithm loosely modelled on neurons in the human brain.3 . The deep learning model I used for this project is based on a model from a lesson in the Codecademy course, Build Deep Learning Models with TensorFlow. This model is coded in a programming language called Python, and uses a deep learning framework from Google, known as Keras (TensorFlow). . My aim was to assemble a model that takes the Brennan dataset as an input, and outputs accurate predictions regarding his voting. . Critically, the model is not pre-programmed with any particular patterns, rules, or guidelines specific to Justice Brennan and the way he voted on SCOTUS. Rather, the model applies the deep learning algorithm to process the dataset and develop, independently, its own &quot;understanding&quot; of his voting history. Based on this understanding, it make predictions. . Forgive me for glossing over a lot of details. But, in simple terms, this is how the model in this project works: . It randomly apportions the dataset into two subsets: one for training, and another for testing (70% for training, 30% for testing). | Then it looks at each case in the training dataset, one-by-one. | With every case, it considers each of the variables (petitioner, respondent, etc), and then predicts whether Justice Brennan voted with the majority or the minority in this particular case. | The model then checks the final column of the dataset: was the prediction correct or not? | It then recalibrates its weighting of each variable based on whether it made a correct or incorrect prediction. When a model works well, this recalibration results in incrementally better (more accurate) predictions. | Once the model has reviewed each of the cases in the training dataset, it then tests itself, case-by-case again, against the second (testing) dataset. This is an important way check against the model merely memorizing the training dataset, as opposed to calibrating its predictive process to enable it to generalize and make accurate predictions about new cases (the test dataset). | After the model cycles through both the training and the testing datasets, it repeats this process over again. Models will do this cycle many times (one hundred, in this instance - but it can be much more). Ideally, the predictive accuracy of the model increases each cycle until it plateaus when it reaches its predictive potential. | . The Results . So how did the model do? . Not too badly. It learned to predict accurately whether Justice Brennan voted with the majority or the minority of the SCOTUS panel with slightly less than 80% accuracy. . If you are interested in the code and the output it produces, click here. . The Real World . Of course, in the &quot;real world&quot; we do not have the benefit of a judge&#39;s entire voting record. Rather, we would have a record of previous votes, from which we would want to predict future votes. . This is the approach taken in Prof. Alschner&#39;s lesson. The dataset is not randomly split. Rather, the machine learning algorithm is trained on the voting record from 1956 until 1979, and then tested against the record from the 1980s. That is, the pre-1980 voting record is used to predict the votes from the 1980s. . How did my deep learning model do when the dataset was likewise apportioned? Performance dropped. At best, the model achieved about 69% accuracy. . I tweeted about this and Prof. Alschner kindly commented, noting the pre-1980 variables seem to miss something important for predicting the post-1980 voting. . This made me do some digging that I should have done at the outset (in my defence, I am a hobbyist who did not know any better at the time!). . Below is a graph showing Justice Brennan&#39;s votes over the entire dataset (on the x axis, 0 means a minority vote and 1 means a majority vote): . . As you can see, Justice Brennan voted with the majority slightly less than 80% of the time over the course of his entire SCOTUS tenure. . When we look at his record from the 1950s until the end of the 1970s, we see that he voted with the majority slightly more than 80% of the time: . . Finally, let&#39;s look at his voting record for the 1980s only: . . We now see a big drop! He voted with the majority less than 70% of the time. This may explain why the model&#39;s prediction accuracy dropped when the dataset was split to train on the pre-1980s data and test on the 1980s data. . Concluding Thoughts . Some final thoughts: . Sometimes a simple approach will yield valuable insight. Just graphing Justice&#39;s Brennan&#39;s voting history provides enough information to make some reasonably accurate predictions. No fancy machine learning algorithms required. | That said, I still consider this experiment a success. The model appears to work, for one thing. At the outset, it &quot;knows&quot; nothing about Justice Brennan&#39;s voting record; it is simply programmed to process the dataset in a certain way. On completion, the model provides reasonably accurate predictions - better than a coin toss, in any event! | What about using these techniques in legal practice? Litigation analytics is already &quot;a thing&quot;, especially in the US. In Canada, unfortunately, we suffer from a deficit of publicly available litigation data, so progress is much slower. For example, do not have an equivalent to the Supreme Court Database, discussed above, for data about the Supreme Court of Canada. | . Thanks for reading! Did I make a mistake? Does something not make sense? Please reach out. . 1. These algorithms are naive bayes, support vector machines, and K-nearest neighbor.↩ . 2. Andrew Trask, Grokking Deep Learning (Shelter Island, NY: Manning Publications, 2019), p. 11↩ . 3. Ibid., p. 10↩ .",
            "url": "http://www.litkm.com/artificial%20intelligence/machine%20learning/deep%20learning/litigation/analytics/python/keras/neural%20networks/law/general/2021/02/11/AI_and_SCOTUS_Judging.html",
            "relUrl": "/artificial%20intelligence/machine%20learning/deep%20learning/litigation/analytics/python/keras/neural%20networks/law/general/2021/02/11/AI_and_SCOTUS_Judging.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "ghtop redux",
            "content": ". Introduction . We recently refactored the CLI tool ghtop, created by the CEO of GitHub, Nat Friedman. Nat even described our refactor as a “tour de force”. This post describes what we learned along the way. . Motivation . Recently, we released ghapi, a new python client for the GitHub API. ghapi provides unparalleled ease of access to the GitHub api, as well as utilities for interacting with GitHub Actions. Part of our motivation for creating ghapi was to accelerate the development of build, testing and deployment tools that help us in maintaining fastai projects. . We recently started using GitHub Actions to perform a wide variety of tasks automatically like: unit and integration tests, deploying documentation, building Docker containers and Conda packages, sharing releases on Twitter, and much more. This automation is key to maintaining the vast open source fastai ecosystem with very few maintainers. . Since ghapi is central to so many of these tasks, we wanted to stress-test its efficacy against other projects. That’s when we found ghtop. This tool allows you to stream all the public events happening on GitHub to a CLI dashboard. We thought it would be a fun learning experience to refactor this code base with various fastai tools such as ghapi and fastcore, but also try out new libraries like rich. . Features we added to our tools . While exploring ghtop, we added several features to various fastai tools that we found to be generally useful. . ghapi Authentication . We added the function github_auth_device which allows users to authenticate their api client with GitHub interactively in a browser. When we call this function we get the following prompt: . github_auth_device() . First copy your one-time code: 276E-C910 Then visit https://github.com/login/device in your browser, and paste the code when prompted. Shall we try to open the link for you? [y/n] . The browser opens a window that looks like this: . . The function then returns an authenticated token which you can use for various tasks. While this is not the only way to create a token, this is a user friendly way to create a token, especially for those who are not as familiar with GitHub. . ghapi Events . As a result of our explorations with ghtop, we added an event module to ghapi. This is useful for retrieving and inspecting sample events. Inspecting sample events is important as it allows you to prototype GitHub Actions workflows locally. You can sample real events with load_sample_events: . from ghapi.event import load_sample_events evts = load_sample_events() . Individual events are formatted as markdown lists to be human readable in Jupyter: . print(evts[0]) . - id: 14517925737 - type: PushEvent - actor: - id: 17030246 - login: BeckhamL - display_login: BeckhamL - gravatar_id: - url: https://api.github.com/users/BeckhamL - avatar_url: https://avatars.githubusercontent.com/u/17030246? - repo: - id: 154349747 - name: BeckhamL/leetcode - url: https://api.github.com/repos/BeckhamL/leetcode - payload: - push_id: 6194986903 - size: 1 - distinct_size: 1 - ref: refs/heads/master - head: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - before: cb16921949c969b5153a0c23ce8fe516d2c8d773 - commits: - - sha: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - author: - email: beckham.lam@mail.mcgill.ca - name: Beckham Lam - message: Create detectCapital.ts - distinct: True - url: https://api.github.com/repos/BeckhamL/leetcode/commits/2055b0fcf22f1c3543e38b60199f6882266d32a5 - public: True - created_at: 2020-12-13T21:32:34Z . You can also inspect the json data in an event, which are accessible as attributes: . evts[0].type . &#39;PushEvent&#39; . For example, here is the frequency of all full_types in the sample: . x,y = zip(*Counter([o.full_type for o in evts]).most_common()) plt.figure(figsize=(8, 6)) plt.barh(x[::-1],y[::-1]); . We can fetch public events in parallel with GhApi.list_events_parallel. In our experiments, repeatedly calling list_events_parallel is fast enough to fetch all current public activity from all users across the entire GitHub platform. We use this for ghtop. Behind the scenes, list_events_parallel uses Python&#39;s ThreadPoolExecutor to fetch events in parallel - no fancy distributed systems or complicated infrastructure necessary, even at the scale of GitHub! . %time api = GhApi() evts = api.list_events_parallel() len(evts) . CPU times: user 2 µs, sys: 0 ns, total: 2 µs Wall time: 4.29 µs . 240 . Note that the GitHub API is stateless, so successive calls to the API will likely return events already seen. We handle this by using a set operations to filter out events already seen. . ghapi pagination . One of the most cumbersome aspects of fetching lots of data from the GitHub api can be pagination. As mentioned in the documentation, different endpoints have different pagination rules and defaults. Therefore, many api clients offer clunky or incomplete interfaces for pagination. . In ghapi we added an entire module with various tools to make paging easier. Below is an example for retrieving repos for the github org. Without pagination, we can only retrieve a fixed number at a time (by default 30): . api = GhApi() repos = api.repos.list_for_org(&#39;fastai&#39;) len(repos) . 30 . However, to get more we can paginate through paged: . from ghapi.event import paged repos = paged(api.repos.list_for_org, &#39;fastai&#39;) for page in repos: print(len(page), page[0].name) . 30 fast-image 30 fastforest 30 .github 8 tweetrel . You can learn more about this functionality by reading the docs. . fastcore Sparklines . Part of goals for refactoring ghtop were to introduce cool visualizations in the terminal of data. We drew inspiration from projects like bashtop, which have CLI interfaces that look like this: . Concretely, we really liked the idea of sparklines in the terminal. Therefore, we created the ability to show sparklines with fastcore: . from fastcore.utils import sparkline data = [9,6,None,1,4,0,8,15,10] print(f&#39;without &quot;empty_zero&quot;: {sparkline(data, empty_zero=False)}&#39;) print(f&#39; with &quot;empty_zero&quot;: {sparkline(data, empty_zero=True )}&#39;) . without &#34;empty_zero&#34;: ▅▂ ▁▂▁▃▇▅ with &#34;empty_zero&#34;: ▅▂ ▁▂ ▃▇▅ . For more information on this function, read the docs. Later in this post, we will describe how we used Rich to add color and animation to these sparklines. . fastcore EventTimer . Because we wanted streaming event data to automatically populate sparklines, we created EventTimer that constructs a histogram according to a frequency and time span you set. With EventTimer, you can add events with add, and get the number of events and their frequency: . from fastcore.utils import EventTimer from time import sleep import random def _randwait(): yield from (sleep(random.random()/200) for _ in range(100)) c = EventTimer(store=5, span=0.03) for o in _randwait(): c.add(1) print(f&#39;Num Events: {c.events}, Freq/sec: {c.freq:.01f}&#39;) print(&#39;Most recent: &#39;, sparkline(c.hist), *L(c.hist).map(&#39;{:.01f}&#39;)) . Num Events: 6, Freq/sec: 301.1 Most recent: ▃▁▁▇▁ 323.6 274.8 291.3 390.9 283.6 . For more information, see the docs. . CLI Animations With Rich . Rich is an amazing python library that allows you to create beautiful, animated and interactive CLI interfaces. Below is a preview of some its features: . Rich also offers animated elements like spinners: . ... and progress bars: . While this post is not about rich, we highly recommend visiting the repo and the docs to learn more. Rich allows you to create your own custom elements. We created two custom elements - Stats and FixedPanel, which we describe below: . Stats: Sparklines with metrics . Stats renders a group of sparklines along with a spinner and a progress bar. First we define our sparklines, the last argument being a list of event types to count: . from ghtop.richext import * from ghtop.all_rich import * console = Console() s1 = ESpark(&#39;Issues&#39;, &#39;green&#39;, [IssueCommentEvent, IssuesEvent]) s2 = ESpark(&#39;PR&#39;, &#39;red&#39;, [PullRequestEvent, PullRequestReviewCommentEvent, PullRequestReviewEvent]) s3 = ESpark(&#39;Follow&#39;, &#39;blue&#39;, [WatchEvent, StarEvent]) s4 = ESpark(&#39;Other&#39;, &#39;red&#39;) s = Stats([s1,s2,s3,s4], store=5, span=.1, stacked=True) console.print(s) . 🌍 Issues PR Follow Other Quota /min 0.0 0.0 0.0 0.0 ━━━━━━━ 0% . You can add events to update counters and sparklines with add_events: . evts = load_sample_events() s.add_events(evts) console.print(s) . 🌍 Issues PR Follow Other Quota /min 11772 ▁▇ 16546 ▁▇ 5991 ▁▇ 6484 ▁ ━━━━━━━ 0% . You can update the progress bar with the update_prog method: . s.update_prog(50) console.print(s) . 🌍 Issues PR Follow Other Quota /min 4076 ▁▇ 5408 ▁▇ 1834 ▁▇ 5998 ▁ ━━━╸━━━ 50% . Here is what the animated version looks like: . . FixedPanel: A panel with fixed height . A key aspect of ghtop is showing events in different panels. We created FixedPanel to allow us to arrange panels in a grid that we can incrementally add events to: . p = FixedPanel(15, box=box.HORIZONTALS, title=&#39;ghtop&#39;) for e in evts: p.append(e) grid([[p,p]]) . ─────────────────── ghtop ─────────────────── ────────────────── ghtop ─────────────────── 📪 dependabo…closed PR #3 o…herzli…&quot;Bump … 📪 dependabo…closed PR #3 …herzli…&quot;Bump … ⭐ dongjun13 pushed 1 commi…dongjun13/2 ⭐ dongjun13 pushed 1 comm…dongjun13/2 ⭐ admmonito…pushed 1 commi…admmonitors/t… ⭐ admmonito…pushed 1 comm…admmonitors/t… ⭐ randomper…pushed 1 commi…randomperson1… ⭐ randomper…pushed 1 comm…randomperson1… ⭐ ahocevar pushed 6 commi…openlayers/ope… ⭐ ahocevar pushed 6 commi…openlayers/op… 🏭 arjmoto created branch …arjmoto/redux-… 🏭 arjmoto created branch…arjmoto/redux-… 💬 stale[bot…created commen…ironha…&quot;This … 💬 stale[bot…created comme…ironha…&quot;This … ⭐ commit-b0…pushed 1 commi…commit-b0t/co… ⭐ commit-b0…pushed 1 comm…commit-b0t/co… ⭐ yakirgot pushed 2 commi…yakirgot/snake ⭐ yakirgot pushed 2 commi…yakirgot/snake 💬 awolf78 created comment…Impulse…&quot;If yo… 💬 awolf78 created commen…Impulse…&quot;If yo… ⭐ kreus7 pushed 1 commit…kreus7/kreusada… ⭐ kreus7 pushed 1 commit…kreus7/kreusad… ⭐ rgripper pushed 1 commi…rgripper/webco… ⭐ rgripper pushed 1 commi…rgripper/webc… 👀 thelittle…started watchi…ritchie46/pol… 👀 thelittle…started watch…ritchie46/pol… 🏭 adrian698 created branch…adrian698/Test 🏭 adrian698 created branc…adrian698/Test ⭐ mergify[b…pushed 2 commi…spbu-coding/6… ⭐ mergify[b…pushed 2 comm…spbu-coding/6… ───────────────────────────────────────────── ──────────────────────────────────────────── . To learn more about our extensions to rich see these docs. . A demo of ghtop animations . Putting all of this together, we get the following results: . 4 Panels with a sparkline for different types of events: . . single panel with a sparkline . . To learn more about ghtop, see the docs. . Interesting python features used . While making these docs, we used the following python features that at least one person we demoed it to found interesting or didn&#39;t know about. If you have been using python for sometime, you might know about all or most of these features: . yield from . Generators are a powerful feature of python, which are especially useful for iterating through large datasets lazily. . dequeue . f-strings .",
            "url": "http://www.litkm.com/ghtop",
            "relUrl": "/ghtop",
            "date": " • Jan 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! I’m Charles Dobson, former litigator, now litigation knowledge management lawyer at a law firm in Toronto, Canada. . The legal world is excited about artificial intelligence and developing better data-driven approaches to practice, project management, and business development. I am curious about how AI works “under the hood”, especially machine learning. I am particularly interested in experimenting with using machine learning in litigation, whether for analytics, research, or pricing. . This blog will consist mainly of technical posts where I puzzle my way through concepts and code, but I am also aiming to include a law angle as often as possible. . If you are working on similiar projects or otherwise share this cross-disclipinary interest, please reach out! You can find me on Twitter or LinkedIn. .",
          "url": "http://www.litkm.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://www.litkm.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}